# YOLOv13n-Face - Utilisant les modules existants du fork Ultralytics 8.3.148

# Model configuration
nc: 1  # number of classes (face)
scales: # model compound scaling constants
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv13n-face
  s: [0.33, 0.50, 1024]  # YOLOv13s-face
  m: [0.67, 0.75, 768]   # YOLOv13m-face
  l: [1.00, 1.00, 512]   # YOLOv13l-face
  x: [1.33, 1.25, 512]   # YOLOv13x-face

# YOLOv13 backbone avec modules existants
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]  # 2
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]  # 4
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2fPSA, [512]]  # 6 - Position Sensitive Attention
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2fPSA, [1024]]  # 8 - Position Sensitive Attention
  - [-1, 1, AIFI, [1024, 8, 1024]]  # 9 - Transformer avec positional embeddings
  - [-1, 1, SPPELAN, [1024, 512, 512]]  # 10 - SPP-ELAN (similaire Ã  RELAN)

# YOLOv13 head avec modules d'attention
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 11
  - [[-1, 6], 1, Concat, [1]]  # 12 cat backbone P4
  - [-1, 3, C2fAttn, [512, 128, 8, 512]]  # 13 - C2f avec attention
  
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 14
  - [[-1, 4], 1, Concat, [1]]  # 15 cat backbone P3
  - [-1, 3, C2f, [256, False]]  # 16 (P3/8-small)
  - [-1, 1, PSA, [256, 256]]  # 17 - Position Sensitive Attention
  
  - [-1, 1, Conv, [256, 3, 2]]  # 18
  - [[-1, 13], 1, Concat, [1]]  # 19 cat head P4
  - [-1, 3, C2f, [512, False]]  # 20 (P4/16-medium)
  - [-1, 1, A2C2f, [512, 512, 1, True, 4]]  # 21 - Area Attention C2f
  
  - [-1, 1, Conv, [512, 3, 2]]  # 22
  - [[-1, 10], 1, Concat, [1]]  # 23 cat head P5
  - [-1, 3, C2f, [1024, False]]  # 24 (P5/32-large)
  
  # Detection head
  - [[17, 21, 24], 1, Detect, [nc]]  # 25 Detect(P3, P4, P5)